
上帝不会告诉我们规律，而是展示给我们数据。

# 数据挖掘的基本流程
商业理解：要从商业的角度理解项目需求，在这个基础上，再对数据挖掘的目标进行定义。
数据理解：尝试收集部分数据，然后对数据进行探索，包括数据描述、数据质量验证等
数据准备：开始收集数据，并对数据进行清洗、数据集成等操作，完成数据挖掘前的准备工作。
模型建立：选择和应用各种数据挖掘模型，并进行优化，以便得到更好的分类结果。
模型评估：对模型进行评价，并检查构建模型的每个步骤，确认模型是否实现了预定的商业目标。
上线发布：模型的作用是从数据中找到金矿，也就是我们所说的“知识”，获得的知识需要转化成用户可以使用的方式，呈现的形式可以是一份报告，也可以是实现一个比较复杂的、可重复的数据挖掘过程。数据挖掘结果如果是日常运营的一部分，那么后续的监控和维护就会变得重要。

# 数据挖掘十大算法
分类算法：C4.5，朴素贝叶斯（Naive Bayes），SVM，KNN，Adaboost，CART
聚类算法：K-Means，EM
关联分析：Apriori
连接分析：PageRank

C4.5 算法是得票最高的算法，可以说是十大算法之首。C4.5 是决策树的算法，它创造性地在决策树构造过程中就进行了剪枝，并且可以处理连续的属性，也能对不完整的数据进行处理。它可以说是决策树分类中，具有里程碑式意义的算法。
朴素贝叶斯模型是基于概率论的原理，它的思想是这样的：对于给出的未知物体想要进行分类，就需要求解在这个未知物体出现的条件下各个类别出现的概率，哪个最大，就认为这个未知物体属于哪个分类。
SVM 的中文叫支持向量机，英文是 Support Vector Machine，简称 SVM。SVM 在训练中建立了一个超平面的分类模型。如果你对超平面不理解，没有关系，我在后面的算法篇会给你进行介绍。
KNN 也叫 K 最近邻算法，英文是 K-Nearest Neighbor。所谓 K 近邻，就是每个样本都可以用它最接近的 K 个邻居来代表。如果一个样本，它的 K 个最接近的邻居都属于分类 A，那么这个样本也属于分类 A。
Adaboost 在训练中建立了一个联合的分类模型。boost 在英文中代表提升的意思，所以 Adaboost 是个构建分类器的提升算法。它可以让我们多个弱的分类器组成一个强的分类器，所以 Adaboost 也是一个常用的分类算法。
CART 代表分类和回归树，英文是 Classification and Regression Trees。像英文一样，它构建了两棵树：一棵是分类树，另一个是回归树。和 C4.5 一样，它是一个决策树学习方法。
Apriori 是一种挖掘关联规则（association rules）的算法，它通过挖掘频繁项集（frequent item sets）来揭示物品之间的关联关系，被广泛应用到商业挖掘和网络安全等领域中。频繁项集是指经常出现在一起的物品的集合，关联规则暗示着两种物品之间可能存在很强的关系。
K-Means 算法是一个聚类算法。你可以这么理解，最终我想把物体划分成 K 类。假设每个类别里面，都有个“中心点”，即意见领袖，它是这个类别的核心。现在我有一个新点要归类，这时候就只要计算这个新点与 K 个中心点的距离，距离哪个中心点近，就变成了哪个类别。
EM 算法也叫最大期望算法，是求参数的最大似然估计的一种方法。原理是这样的：假设我们想要评估参数 A 和参数 B，在开始状态下二者都是未知的，并且知道了 A 的信息就可以得到 B 的信息，反过来知道了 B 也就得到了 A。可以考虑首先赋予 A 某个初值，以此得到 B 的估值，然后从 B 的估值出发，重新估计 A 的取值，这个过程一直持续到收敛为止。
PageRank 起源于论文影响力的计算方式，如果一篇文论被引入的次数越多，就代表这篇论文的影响力越强。同样 PageRank 被 Google 创造性地应用到了网页权重的计算中：当一个页面链出的页面越多，说明这个页面的“参考文献”越多，当这个页面被链入的频率越高，说明这个页面被引用的次数越高。基于这个原理，我们可以得到网站的权重划分。

# 数据挖掘的数学原理
概率论与数理统计
线性代数
图论
最优化方法


#Apriori算法
通过分析购物篮商品的集合，找出商品间的关联关系。利用隐性的关联关系，强化这类购买行为，提升销售额。

# 商业智能BI 数据仓库DW 数据挖掘DM
数据挖掘的核心包括分类、聚类、预测、关联分析等

元数据(MetaData):描述其他数据的数据
数据元(Data Element):最小数据单元

Knowledge Discovery in Database KDD,数据库中的直属发现

分类：就是通过训练集得到一个分类模型，然后用这个模型可以对其他数据进行分类。
聚类：就是将数据自动聚类成几个类别，聚到一起的相似度大，不在一起的差异性大。我们往往利用聚类来做数据划分。
预测：就是通过当前和历史数据来预测未来趋势，它可以更好地帮助我们识别机遇和风险。
关联分析：就是发现数据中的关联规则，它被广泛应用在购物篮分析，或事务数据分析中。

首先，输入我们收集到的数据，然后对数据进行预处理。预处理通常是将数据转化成我们想要的格式，然后我们再对数据进行挖掘，最后通过后处理得到我们想要的信息。
数据预处理中，我们会对数据进行几个处理步骤：数据清洗，数据集成，以及数据变换。
数据后处理是将模型预测的结果进一步处理后，再导出。比如在二分类问题中，一般能得到的是 0~1 之间的概率值，此时把数据以 0.5 为界限进行四舍五入就可以实现后处理。

# 用户画像
精细化运营将是长久的主题

## 用户画像的准则
统一化 标签化 业务化
用户唯一标识是整个用户画像的核心。
--设计唯一标识可以从这些项中选择：用户名、注册手机号、联系人手机号、邮箱、设备号、CookieID 等。
其次，给用户打标签。
--"用户消费行为分析"
用户标签：它包括了性别、年龄、地域、收入、学历、职业等。这些包括了用户的基础属性
消费标签：消费习惯、购买意向、是否对促销敏感。这些统计分析用户的消费习惯。
行为标签：时间段、频次、时长、访问路径。这些是通过分析用户行为，来得到他们使用 App 的习惯。
内容分析：对用户平时浏览的内容，尤其是停留时间长、浏览次数多的内容进行分析，分析出用户对哪些内容感兴趣，比如，金融、娱乐、教育、体育、时尚、科技等。

用户画像就是用户的数学建模，们正是将海量数据进行标签化，来得到精准的用户画像，从而为企业更精准地解决问题。

目的：获客、粘客和留客
获客：如何进行拉新，通过更精准的营销获取客户。
粘客：个性化推荐，搜索排序，场景运营等。
留客：流失率预测，分析关键节点降低流失率。

数据层指的是用户消费行为里的标签。我们可以打上“事实标签”，作为数据客观的记录。
算法层指的是透过这些行为算出的用户建模。我们可以打上“模型标签”，作为用户画像的分类标识。
业务层指的是获客、粘客、留客的手段。我们可以打上“预测标签”，作为业务关联的结果。


比如一个经常买沙拉的人，一般很少吃夜宵。同样，一个经常吃夜宵的人，吃小龙虾的概率可能远高于其他人。这些结果都是通过数据挖掘中的关联分析得出的。
在获客上，我们可以找到优势的宣传渠道，如何通过个性化的宣传手段，吸引有潜在需求的用户，并刺激其转化。
在粘客上，如何提升用户的单价和消费频次，方法可以包括购买后的个性化推荐、针对优质用户进行优质高价商品的推荐、以及重复购买，比如通过红包、优惠等方式激励对优惠敏感的人群，提升购买频次。
在留客上，预测用户是否可能会从平台上流失。在营销领域，关于用户留存有一个观点——如果将顾客流失率降低 5%，公司利润将提升 25%~85%。可以看出留存率是多么的重要。用户流失可能会包括多种情况，比如用户体验、竞争对手、需求变化等，通过预测用户的流失率可以大幅降低用户留存的运营成本。

## 锻炼自己的抽象能力，将繁杂的事务简单化
在做用户画像的时候，还要考虑到异常值的处理。
我们的最终目的不是处理这些数据，而是理解、使用这些数据挖掘的结果。

# 自动化采集数据
多源的数据采集 尽可能多的数据维度 保证数据质量

开放数据源：开放数据源一般是针对行业的数据库。比如美国人口调查局开放了美国的人口信息、地区分布和教育情况数据。除了政府外，企业和高校也会开放相应的大数据，这方面北美相对来说做得好一些。国内，贵州做了不少大胆尝试，搭建了云平台，逐年开放了旅游、交通、商务等领域的数据量。
爬虫抓取：一般是针对特定的网站或 App。如果我们想要抓取指定的网站数据，比如购物网站上的购物评价等，就需要我们做特定的爬虫抓取。
日志采集：这个是统计用户的操作。我们可以在前端进行埋点，在后端进行脚本收集、统计，来分析网站的访问情况，以及使用瓶颈等。
传感器：它基本上采集的是物理信息。比如图像、视频、或者某个物体的速度、热度、压强等。

## 开放数据
美国人口调查局 http://www.census.gov/data.html 提供人口信息、地区分布和教育情况等美国公民相关数据
欧盟 http://open-data.europa.eu/en/data/ 欧盟开放数据平台，提供欧盟各机构的大量数据
Facebook https://developers.facebook.com/docs/graph-api Facebook官方api，用于查询该网站用户公开的海量数据
Amazon http://aws.amazon.com/datasets 亚马逊网络服务开放数据集
Google https://www.google.com/finance 谷歌金融，收录了40年来的股票数据，实时更新
北京大学 http://opendata.pku.edu.cn/ 北京大学开放研究数据平台
ImageNet http://www.image-net.org/ 目前世界上最大图像识别最大的数据库，包括近1500万张图像

## python 爬虫
1.Requests爬取内容
2.使用XPath解析内容
3.使用pandas保存数据

scrapy爬虫框架

火车采集器
八爪鱼
集搜客
**很多时候自动切换 IP 以及云采集才是自动化采集的关键。

日志采集：
通过 Web 服务器采集，例如 httpd、Nginx、Tomcat 都自带日志记录功能。同时很多互联网企业都有自己的海量数据采集工具，多用于系统日志采集，如 Hadoop 的 Chukwa、Cloudera 的 Flume、Facebook 的 Scribe 等，这些工具均采用分布式架构，能够满足每秒数百 MB 的日志数据采集和传输需求。
自定义采集用户行为，例如用 JavaScript 代码监听用户的行为、AJAX 异步请求后台记录日志等。

埋点就是在有需要的位置采集相应的信息，进行上报。比如某页面的访问情况，包括用户信息、设备信息；或者用户在页面上的操作行为，包括时间长短等。这就是埋点，每一个埋点就像一台摄像头，采集用户行为数据，将数据进行多维度的交叉分析，可真实还原出用户使用场景，和用户使用需求。

### 爬虫流程
Requests访问页面：

Xpath定位：
node 选node节点的所有子节点
/ 从根节点选取
// 选取所有的当前节点，不考虑他们的位置
. 当前节点
.. 父节点
@ 属性选择
| 或 两个节点的合计
text() 当前路径下的文本内容

JSON对象：

# 数据清洗
好的数据分析师必定是一名数据清洗高手，要知道在整个数据分析过程中，不论是在时间还是功夫上，数据清洗大概都占到了 80%。

## 数据质量的准则
完整性：单条数据是否存在空值，统计的字段是否完善
全面性：观察某一列的全部数值，比如在 Excel 表中，我们选中一列，可以看到该列的平均值、最大值、最小值。我们可以通过常识来判断该列是否有问题，比如：数据定义、单位标识、数值本身。
合法性：数据的类型、内容、大小的合法性。比如数据中存在非 ASCII 字符，性别存在了未知，年龄超过了 150 岁等。
唯一性：据是否存在重复记录，因为数据通常来自不同渠道的汇总，重复的情况是常见的。行数据、列数据都需要是唯一的，比如一个人不能重复记录多次，且一个人的体重也不能在列指标中重复记录多次。

数据标准、干净、连续

## 清洗数据，一一击破

### 完整性
缺失值：删除、均值、高频
空行：一般删除
### 全面性
列数据的单位不统一：单位统一
### 合理性
非ASCII字符
### 唯一性
一列有多个参数：切分字段、合并字段
重复数据：删除重复数据

## 养成数据审核的习惯
没有高质量的数据，就没有高质量的数据挖掘，而数据清洗是高质量数据的一道保障。

# 数据集成
两种架构：ETL 和 ELT
ETL: Extract Transform Load 即数据抽取 转换 加载

区别：
ELT和ETL相比，最大的区别是“重抽取和加载，轻转换”，从而可以用更轻量级的方案搭建一个数据集成平台。一方面更省时，另一方面 ELT 允许 BI 分析人员无限制地访问整个原始数据，为分析师提供了更大的灵活性，使之能更好地支持业务。
在ELT架构中，数据变换这个过程根据后续使用的情况，需要再SQL中进行，而不是在加载阶段进行。这样做的好处是你可以从数据源中提取数据，经过少量预处理后进行加载。这样的架构更简单，使分析人员更好地了解原始数据的变换过程。

## ETL工具
商业软件：Informatica PowerCenter、IBM InfoSphere DataStage、Oracle Data Integrator、Microsoft SQL Server Integration Services 等
开源软件：Kettle、Talend、Apatar、Scriptella、DataX、Sqoop 等

### Kettle
Kettle 是一款国外开源的 ETL 工具，纯 Java 编写，可以在 Window 和 Linux 上运行，不需要安装就可以使用。
Kettle 在 2006 年并入了开源的商业智能公司 Pentaho, 正式命名为 Pentaho Data Integeration，简称“PDI”。因此 Kettle 现在是 Pentaho 的一个组件
https://community.hitachivantara.com/docs/DOC-1009855
http://www.kettle.net.cn/

transformation转换和job作业
Transformation（转换）：相当于一个容器，对数据操作进行了定义。数据操作就是数据从输入到输出的一个过程。你可以把转换理解成为是比作业粒度更小的容器。在通常的工作中，我们会把任务分解成为不同的作业，然后再把作业分解成多个转换。
Job（作业）：相比于转换是个更大的容器，它负责将转换组织起来完成某项作业。

transformation:Step(步骤) Hop(跳跃线，即数据流向)
Job:Jont Entry、 Hop
Job Entry 是 Job 内部的执行单元，每一个 Job Entry 都是用来执行具体的任务，比如调用转换，发送邮件等。
Hop：指连接 Job Entry 的线。并且它可以指定是否有条件地执行。

### DataX
将自己作为标准，连接了不同的数据源，以完成它们之间的转换。
DataX 的模式是基于框架 + 插件完成的，DataX 的框架如下图：
在这个框架里，Job 作业被 Splitter 分割器分成了许多小作业 Sub-Job。在 DataX 里，通过两个线程缓冲池来完成读和写的操作，读和写都是通过 Storage 完成数据的交换。比如在“读”模块，切分后的小作业，将数据从源头装载到 DataXStorage，然后在“写”模块，数据从 DataXStorage 导入到目的地。

### Apache 开源软件:Sqoop
Sqoop 在 Hadoop 生态系统中是占据一席之地的，它主要用来在 Hadoop 和关系型数据库中传递数据。通过 Sqoop，我们可以方便地将数据从关系型数据库导入到 HDFS 中，或者将数据从 HDFS 导出到关系型数据库中。
